# funcoes_globais.py (vers√£o final com retry de blocos)

import logging
import time
import pandas as pd
import os
import pyodbc
from urllib.parse import quote_plus
from sqlalchemy import create_engine, text
import numpy as np
import math

logger = logging.getLogger("logger_financa")

# Suas importa√ß√µes personalizadas
from conexoes import CONEXOES
from consultas_definidas import consultas
from criador_dataframe import CriadorDataFrame

def funcao_conexao(nome_conexao: str, tentativas: int = 3, delay_segundos: int = 10) -> create_engine:
    """
    Cria uma engine com retry, timeout de query e reciclagem de conex√£o.
    """
    for tentativa in range(tentativas):
        try:
            info = CONEXOES[nome_conexao]
            tipo_conexao = info.get("tipo")
            
            odbc_str = ""
            if tipo_conexao == "sql":
                servidor = info["servidor"]
                banco = info["banco"]
                driver = info["driver"].replace('+', ' ')
                trusted_str = "Trusted_Connection=yes;" if info.get("trusted_connection", False) else ""
                odbc_str = f"DRIVER={{{driver}}};SERVER={servidor};DATABASE={banco};{trusted_str};timeout=600"
            
            elif tipo_conexao == "olap":
                return info["str_conexao"]
            else:
                raise ValueError(f"Tipo de conex√£o '{tipo_conexao}' n√£o suportado.")

            string_conexao = f"mssql+pyodbc:///?odbc_connect={quote_plus(odbc_str)}"
            
            engine = create_engine(
                string_conexao, 
                pool_pre_ping=True, 
                pool_recycle=300
            )
            
            with engine.connect() as connection:
                logger.info(f"‚úÖ Conex√£o com '{nome_conexao}' estabelecida (pool_recycle=300s).")
                return engine

        except pyodbc.OperationalError as e:
            if '08S01' in str(e) and tentativa < tentativas - 1:
                logger.warning(f"‚ö†Ô∏è Falha de comunica√ß√£o ao conectar. Tentando novamente em {delay_segundos}s...")
                time.sleep(delay_segundos)
            else:
                logger.error(f"Erro final de conex√£o na tentativa {tentativa + 1}.", exc_info=True)
                raise e
    raise ConnectionError(f"N√£o foi poss√≠vel conectar a '{nome_conexao}' ap√≥s {tentativas} tentativas.")


def selecionar_consulta_por_nome(titulo: str) -> pd.DataFrame:
    """Executa a consulta pelo nome e retorna um DataFrame."""
    # Esta fun√ß√£o n√£o precisa mais de l√≥gica de retry, pois a causa do erro foi identificada.
    logger.info(f"‚ñ∂Ô∏è Executando a consulta: '{titulo}'...")
    inicio = time.perf_counter()
    try:
        consulta_encontrada = consultas.get(titulo)
        if not consulta_encontrada:
            raise ValueError(f"Consulta '{titulo}' n√£o encontrada nas defini√ß√µes.")

        tipo_correto = "mdx" if consulta_encontrada.tipo == "olap" else consulta_encontrada.tipo
        
        df = CriadorDataFrame(
            funcao_conexao,
            consulta_encontrada.conexao,
            consulta_encontrada.sql,
            tipo_correto
        ).executar()
        
        fim = time.perf_counter()
        tempo = fim - inicio
        if not df.empty:
            logger.info(f"‚úÖ Consulta '{titulo}' finalizada em {tempo:.2f} segundos ({len(df)} linhas).")
        else:
            logger.warning(f"‚ö†Ô∏è Consulta '{titulo}' finalizada, mas n√£o retornou nenhuma linha.")
        return df
        
    except Exception as e:
        logger.error(f"‚ùå Erro fatal ao executar 'selecionar_consulta_por_nome': {e}", exc_info=True)
        return pd.DataFrame()


def salvar_no_financa(df: pd.DataFrame, table_name: str, retries_por_chunk: int = 2):
    """
    Salva o DataFrame de forma resiliente. Tenta salvar cada bloco e, se falhar,
    guarda para uma segunda rodada de tentativas no final.
    """
    if df.empty:
        logger.warning(f"‚ö†Ô∏è DataFrame est√° vazio. Nada ser√° salvo.")
        return

    logger.info(f"üìÄ Iniciando processo de salvamento para a tabela '{table_name}'.")
    inicio_total = time.perf_counter()
    engine = None
    blocos_falhos = [] # Lista para guardar os blocos que falharam

    try:
        engine = funcao_conexao("SPSVSQL39")
        
        SQL_SERVER_PARAM_LIMIT = 2100
        num_colunas = len(df.columns)
        chunksize = (SQL_SERVER_PARAM_LIMIT // num_colunas) if num_colunas > 0 else 1000
        
        total_rows = len(df)
        num_chunks = math.ceil(total_rows / chunksize)
        
        with engine.begin() as connection:
            logger.info(f"üóëÔ∏è Removendo a tabela antiga '{table_name}'...")
            connection.execute(text(f'DROP TABLE IF EXISTS "{table_name}"'))
            logger.info(f"‚úÖ Tabela removida.")
            
        logger.info(f"üíæ Iniciando 1¬™ rodada: salvando {total_rows} linhas em {num_chunks} blocos...")
        chunks = np.array_split(df, num_chunks)

        for i, chunk_df in enumerate(chunks):
            bloco_atual = i + 1
            try:
                with engine.begin() as connection:
                    logger.info(f"  -> 1¬™ Tentativa: Salvando bloco {bloco_atual}/{num_chunks}...")
                    chunk_df.to_sql(name=table_name, con=connection, if_exists='append', index=False, method='multi')
            except pyodbc.OperationalError as e:
                if '08S01' in str(e):
                    logger.warning(f"  ‚ö†Ô∏è Falha de comunica√ß√£o no bloco {bloco_atual}. Adicionando √† lista para retentativa.")
                    blocos_falhos.append((bloco_atual, chunk_df))
                else:
                    logger.error(f"  ‚ùå Erro de banco de dados n√£o recuper√°vel no bloco {bloco_atual}.", exc_info=True)
                    raise e # Falha imediatamente se o erro n√£o for de comunica√ß√£o
        
        # --- SEGUNDA RODADA: TENTA SALVAR NOVAMENTE OS BLOCOS QUE FALHARAM ---
        if blocos_falhos:
            logger.warning(f"--- Iniciando 2¬™ rodada para {len(blocos_falhos)} blocos que falharam ---")
            blocos_com_falha_permanente = []
            
            for bloco_num, chunk_df in blocos_falhos:
                try:
                    with engine.begin() as connection:
                        logger.info(f"  -> 2¬™ Tentativa: Salvando bloco {bloco_num}...")
                        chunk_df.to_sql(name=table_name, con=connection, if_exists='append', index=False, method='multi')
                    logger.info(f"  ‚úÖ Sucesso na 2¬™ tentativa para o bloco {bloco_num}.")
                except Exception as e:
                    logger.error(f"  ‚ùå FALHA PERMANENTE no bloco {bloco_num} mesmo na 2¬™ tentativa.", exc_info=True)
                    blocos_com_falha_permanente.append(bloco_num)

            if blocos_com_falha_permanente:
                # Se ainda houver falhas, levanta uma exce√ß√£o para que a 'main' saiba que o processo n√£o foi 100%
                raise RuntimeError(f"N√£o foi poss√≠vel salvar os seguintes blocos: {blocos_com_falha_permanente}")

        fim_total = time.perf_counter()
        tempo_total = fim_total - inicio_total
        
        if blocos_falhos and not blocos_com_falha_permanente:
             logger.info(f"üéâ Sucesso! Todos os {total_rows} foram salvos, com algumas retentativas, em {tempo_total:.2f} segundos.")
        else:
             logger.info(f"üéâ Sucesso! Todos os {total_rows} foram salvos na primeira rodada em {tempo_total:.2f} segundos.")

    except Exception as e:
        logger.error(f"‚ùå O processo de salvamento falhou. Causa: {e}", exc_info=True)
        raise e
    finally:
        if engine:
            engine.dispose()
